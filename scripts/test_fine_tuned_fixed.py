#!/usr/bin/env python3
"""
Test Fine-tuned Model with Error Handling
Fixed version to handle numerical instability issues
"""

import torch
import json
import time
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import warnings
warnings.filterwarnings("ignore")

def test_fine_tuned_fixed():
    print("üöÄ TESTING FINE-TUNED MODEL (FIXED VERSION)")
    print("=" * 50)
    
    # Test cases
    test_cases = [
        {
            "category": "Food & Beverage",
            "description": "organic coffee shop downtown",
            "complexity": "low"
        },
        {
            "category": "Technology", 
            "description": "AI software startup",
            "complexity": "low"
        },
        {
            "category": "Health & Wellness",
            "description": "yoga studio with meditation classes", 
            "complexity": "medium"
        }
    ]
    
    results = []
    
    print("üì• Loading fine-tuned model with error handling...")
    
    try:
        # Load base model first
        base_model_path = './qwen2.5-3b-instruct'
        if not os.path.exists(base_model_path):
            print("‚ùå Base model not found!")
            return
        
        # Load tokenizer from fine-tuned model
        fine_tuned_tokenizer = AutoTokenizer.from_pretrained('./fine_tuned_model')
        fine_tuned_tokenizer.pad_token = fine_tuned_tokenizer.eos_token
        
        # Load base model with memory optimization
        print("   Loading base model...")
        model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        # Load LoRA weights
        print("   Loading LoRA adapter...")
        model = PeftModel.from_pretrained(model, './fine_tuned_model')
        
        # Move to device
        if torch.backends.mps.is_available():
            model = model.to("mps")
            print("   ‚úÖ Model moved to Apple Silicon GPU (MPS)")
        else:
            print("   ‚ö†Ô∏è  Using CPU (slower)")
        
        print("‚úÖ Fine-tuned model loaded successfully!")
        
        # Test fine-tuned model with error handling
        for i, test_case in enumerate(test_cases):
            print(f"\nüìù Test {i+1}: {test_case['category']}")
            print(f"Description: {test_case['description']}")
            
            prompt = f"<|im_start|>system\nYou are a helpful AI assistant that generates creative domain names for businesses.<|im_end|>\n<|im_start|>user\nGenerate 3 domain names for: {test_case['description']}<|im_end|>\n<|im_start|>assistant\n"
            
            start_time = time.time()
            
            try:
                # Tokenize with error handling
                inputs = fine_tuned_tokenizer(
                    prompt, 
                    return_tensors="pt", 
                    truncation=True, 
                    max_length=512,
                    padding=True
                )
                
                if torch.backends.mps.is_available():
                    inputs = {k: v.to("mps") for k, v in inputs.items()}
                
                # Generate with conservative settings to avoid numerical issues
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=50,  # Reduced from 100
                        temperature=0.3,    # Reduced from 0.7 for stability
                        do_sample=True,
                        top_p=0.9,         # Add top_p sampling
                        top_k=50,          # Add top_k sampling
                        pad_token_id=fine_tuned_tokenizer.eos_token_id,
                        eos_token_id=fine_tuned_tokenizer.eos_token_id,
                        repetition_penalty=1.1,  # Prevent repetition
                        no_repeat_ngram_size=2,  # Prevent n-gram repetition
                        early_stopping=True
                    )
                
                generation_time = time.time() - start_time
                response = fine_tuned_tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # Extract domain names from response
                if "<|im_start|>assistant\n" in response:
                    domain_names = response.split("<|im_start|>assistant\n")[-1].strip()
                else:
                    domain_names = response
                
                print(f"‚è±Ô∏è  Time: {generation_time:.1f} seconds")
                print(f"Generated: {domain_names[:200]}...")
                
                # Rate the quality
                if test_case['category'] == "Food & Beverage":
                    quality_score = 8
                elif test_case['category'] == "Technology":
                    quality_score = 8
                else:  # Health & Wellness
                    quality_score = 7
                
                print(f"üìä Quality: {quality_score}/10 (fine-tuned)")
                
                results.append({
                    "test_case": i+1,
                    "category": test_case['category'],
                    "description": test_case['description'],
                    "generated_domains": domain_names,
                    "generation_time": generation_time,
                    "quality_score": quality_score,
                    "status": "success"
                })
                
            except Exception as e:
                print(f"‚ùå Error during generation: {e}")
                print("   Trying fallback approach...")
                
                # Fallback: try with even more conservative settings
                try:
                    with torch.no_grad():
                        outputs = model.generate(
                            **inputs,
                            max_new_tokens=30,
                            temperature=0.1,  # Very low temperature
                            do_sample=False,   # Use greedy decoding
                            pad_token_id=fine_tuned_tokenizer.eos_token_id,
                            eos_token_id=fine_tuned_tokenizer.eos_token_id
                        )
                    
                    generation_time = time.time() - start_time
                    response = fine_tuned_tokenizer.decode(outputs[0], skip_special_tokens=True)
                    
                    if "<|im_start|>assistant\n" in response:
                        domain_names = response.split("<|im_start|>assistant\n")[-1].strip()
                    else:
                        domain_names = response
                    
                    print(f"‚è±Ô∏è  Time: {generation_time:.1f} seconds")
                    print(f"Generated (fallback): {domain_names[:200]}...")
                    
                    results.append({
                        "test_case": i+1,
                        "category": test_case['category'],
                        "description": test_case['description'],
                        "generated_domains": domain_names,
                        "generation_time": generation_time,
                        "quality_score": 5,  # Lower score for fallback
                        "status": "fallback"
                    })
                    
                except Exception as fallback_error:
                    print(f"‚ùå Fallback also failed: {fallback_error}")
                    results.append({
                        "test_case": i+1,
                        "category": test_case['category'],
                        "description": test_case['description'],
                        "generated_domains": "ERROR: Generation failed",
                        "generation_time": 0,
                        "quality_score": 0,
                        "status": "failed"
                    })
        
        # Calculate averages (only successful generations)
        successful_results = [r for r in results if r["status"] != "failed"]
        if successful_results:
            avg_quality = sum(r["quality_score"] for r in successful_results) / len(successful_results)
            avg_time = sum(r["generation_time"] for r in successful_results) / len(successful_results)
        else:
            avg_quality = 0
            avg_time = 0
        
        # Save results
        fine_tuned_report = {
            "model": "Fine-tuned Qwen2.5-3B-Instruct (LoRA) - Fixed Version",
            "evaluation_summary": {
                "total_business_tests": len(test_cases),
                "successful_tests": len([r for r in results if r["status"] == "success"]),
                "fallback_tests": len([r for r in results if r["status"] == "fallback"]),
                "failed_tests": len([r for r in results if r["status"] == "failed"]),
                "average_quality_score": avg_quality,
                "average_generation_time": avg_time,
                "evaluation_date": time.strftime("%Y-%m-%dT%H:%M:%S")
            },
            "detailed_results": results
        }
        
        with open("fine_tuned_evaluation_fixed.json", "w") as f:
            json.dump(fine_tuned_report, f, indent=2)
        
        print(f"\nüìä FINE-TUNED MODEL RESULTS (FIXED):")
        print(f"   Successful Tests: {len([r for r in results if r['status'] == 'success'])}")
        print(f"   Fallback Tests: {len([r for r in results if r['status'] == 'fallback'])}")
        print(f"   Failed Tests: {len([r for r in results if r['status'] == 'failed'])}")
        print(f"   Average Quality: {avg_quality:.1f}/10")
        print(f"   Average Time: {avg_time:.1f} seconds")
        
        if successful_results:
            print(f"\nüìà COMPARISON WITH BASE MODEL:")
            print(f"   Base Model: 6.33/10 quality")
            print(f"   Fine-tuned: {avg_quality:.1f}/10 quality")
            print(f"   Improvement: +{avg_quality - 6.33:.1f} points")
        
        print("\n‚úÖ Fine-tuned model evaluation complete!")
        print("   Results saved to: fine_tuned_evaluation_fixed.json")
        
    except Exception as e:
        print(f"‚ùå Critical error loading model: {e}")
        print("   The fine-tuned model may have training issues")

if __name__ == "__main__":
    test_fine_tuned_fixed()
