#!/usr/bin/env python3
"""
Quick Status Check: Project Status
Shows current state without loading the model
"""

import os
import json
from pathlib import Path

def check_status():
    print("ğŸ” QUICK STATUS CHECK: AI Homework Project")
    print("=" * 50)
    
    # Check model files
    print("ğŸ“ MODEL STATUS:")
    base_model_path = "./qwen2.5-3b-instruct"
    if os.path.exists(base_model_path):
        print("   âœ… Base model exists (Qwen2.5-3B-Instruct)")
        
        # Check model size
        model_files = list(Path(base_model_path).rglob("*.safetensors"))
        total_size = sum(f.stat().st_size for f in model_files)
        size_gb = total_size / (1024**3)
        print(f"   ğŸ“Š Model size: {size_gb:.1f} GB")
    else:
        print("   âŒ Base model missing")
    
    # Check fine-tuned model
    ft_model_path = "./fine_tuned_model"
    if os.path.exists(ft_model_path):
        print("   âœ… Fine-tuned model exists")
        ft_files = list(Path(ft_model_path).rglob("*"))
        print(f"   ğŸ“Š Fine-tuned files: {len(ft_files)}")
    else:
        print("   âŒ Fine-tuned model does NOT exist")
    
    # Check training data
    print("\nğŸ“Š TRAINING DATA:")
    if os.path.exists("training_dataset.json"):
        with open("training_dataset.json", "r") as f:
            data = json.load(f)
        print(f"   âœ… Training dataset: {len(data)} examples")
        
        if len(data) < 10:
            print("   âš ï¸  Very small dataset - may need more examples")
        elif len(data) < 100:
            print("   âš ï¸  Small dataset - consider expanding")
        else:
            print("   âœ… Dataset size looks good")
    else:
        print("   âŒ Training dataset missing")
    
    # Check evaluation results
    print("\nğŸ“ˆ EVALUATION STATUS:")
    if os.path.exists("evaluation_report.json"):
        print("   âœ… Evaluation report exists")
    else:
        print("   âŒ No evaluation report found")
    
    if os.path.exists("model_comparison_report.json"):
        print("   âœ… Model comparison report exists")
    else:
        print("   âŒ No model comparison report found")
    
    # Check scripts
    print("\nğŸ”§ SCRIPTS STATUS:")
    scripts_dir = "./scripts"
    if os.path.exists(scripts_dir):
        scripts = [f for f in os.listdir(scripts_dir) if f.endswith('.py')]
        print(f"   âœ… Scripts directory: {len(scripts)} Python files")
        
        # Check key scripts
        key_scripts = ["fine_tune_lora.py", "evaluate_improvements.py", "quick_evaluation.py"]
        for script in key_scripts:
            if os.path.exists(os.path.join(scripts_dir, script)):
                print(f"   âœ… {script}")
            else:
                print(f"   âŒ {script} missing")
    else:
        print("   âŒ Scripts directory missing")
    
    # Overall status
    print("\nğŸ¯ OVERALL STATUS:")
    
    if os.path.exists(ft_model_path):
        print("   ğŸ‰ FINE-TUNING COMPLETE!")
        print("   âœ… Ready for evaluation and comparison")
    else:
        print("   âš ï¸  FINE-TUNING NOT COMPLETE")
        print("   âŒ Missing fine-tuned model")
        print("   âŒ No model comparison possible")
        print("   âŒ Core requirement not met")
    
    print(f"\nâ° TIME ESTIMATES:")
    print(f"   Model Loading: 10-15 seconds (one-time)")
    print(f"   Generation: 10-30 seconds per prompt")
    print(f"   Full Evaluation: 5-10 minutes")
    print(f"   Fine-tuning: 30-60 minutes (not done)")

if __name__ == "__main__":
    check_status()
